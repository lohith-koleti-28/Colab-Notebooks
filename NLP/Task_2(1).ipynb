{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes and Vector Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the data and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories: ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "# Load the 20 newsgroups dataset\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = newsgroups.data[:1000], newsgroups.data[1000:], newsgroups.target[:1000], newsgroups.target[1000:]\n",
    "\n",
    "# Categories for evaluation\n",
    "categories = newsgroups.target_names\n",
    "\n",
    "print(f\"Categories: {categories}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Convert text to lowercase and remove punctuation and non alpha numeric characters using regex\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing to training data\n",
    "X_train = [preprocess_text(text) for text in X_train]\n",
    "X_test = [preprocess_text(text) for text in X_test]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here create 2 functions: Bag of Words and TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Bag of Words (BoW)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First create the vocabulary for the bag of words, in layman terms get the set of all unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(text_data):\n",
    "    \"\"\"Create a vocabulary from the provided text data.\"\"\"\n",
    "    vocabulary = set()\n",
    "    for text in text_data:\n",
    "        words = text.split()\n",
    "        vocabulary.update(words)\n",
    "    return list(vocabulary)\n",
    "\n",
    "# Create vocabulary from the training data\n",
    "vocabulary = create_vocabulary(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next create a function to compute the number of occurences of that word given the vocabulary and text, return a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BoW(text, vocabulary):\n",
    "    \"\"\"Convert a text document into a vector based on the vocabulary.\"\"\"\n",
    "    word_count = {word: 0 for word in vocabulary}\n",
    "    words = text.split()\n",
    "    for word in words:\n",
    "        if word in word_count:\n",
    "            word_count[word] += 1\n",
    "    return list(word_count.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **TF-IDF**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF-IDF (Term Frequency-Inverse Document Frequency)** is a numerical statistic used to evaluate the importance of a word within a document relative to a collection (or corpus) of documents. It is commonly used in text mining and information retrieval tasks such as text classification, clustering, and document retrieval.\n",
    "\n",
    "##### 1. **Term Frequency (TF)**\n",
    "\n",
    "Term Frequency measures how frequently a term (word) appears in a document. It is calculated as:\n",
    "\n",
    "$$\n",
    "\\text{TF}(t, d) = \\frac{\\text{Number of times term t appears in document d}}{\\text{Total number of terms in document d}}\n",
    "$$\n",
    "\n",
    "##### 2. **Inverse Document Frequency (IDF)**\n",
    "\n",
    "Inverse Document Frequency measures how important a term is in the entire corpus. It reduces the weight of common words that appear in many documents. The IDF is calculated as:\n",
    "\n",
    "$$\n",
    "\\text{IDF}(t) = \\log \\frac{N}{df(t)}\n",
    "$$\n",
    "\n",
    "- Where:\n",
    "  - $ N $ is the total number of documents in the corpus.\n",
    "  - $ df(t) $ is the number of documents that contain the term \\$$t \\).\n",
    "\n",
    "##### 3. **TF-IDF Score**\n",
    "\n",
    "The **TF-IDF score** is the product of the Term Frequency (TF) and Inverse Document Frequency (IDF):\n",
    "\n",
    "$$\n",
    "\\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\times \\text{IDF}(t)\n",
    "$$\n",
    "\n",
    "- This score reflects the importance of a term in a document relative to its rarity across the entire corpus.\n",
    "\n",
    "\n",
    "##### 4. **Smoothed IDF (Inverse Document Frequency)**\n",
    "\n",
    "The formula for **IDF** can be smoothed by adding a constant to the document frequency count in the denominator. This avoids division by zero and prevents excessively high values for terms that appear in most documents.\n",
    "\n",
    "**Smoothed IDF Formula:**\n",
    "\n",
    "$$\n",
    "\\text{IDF}_{\\text{smooth}}(t) = \\log \\left( \\frac{N + 1}{df(t) + 1} \\right) + 1\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ N $ = Total number of documents in the corpus.\n",
    "- $ df(t) $ = Number of documents that contain the term $ t $.\n",
    "- The `+1` in both the numerator and denominator ensures that the term frequency is never zero.\n",
    "\n",
    "##### 5. **Smoothed TF (Term Frequency)**\n",
    "\n",
    "A common smoothing technique for **Term Frequency** is to apply **logarithmic scaling**. Instead of using the raw term count, the logarithm of the term frequency is used to downscale the impact of very frequent terms.\n",
    "\n",
    "**Smoothed TF Formula:**\n",
    "\n",
    "$$\n",
    "\\text{TF}_{\\text{smooth}}(t, d) = 1 + \\log(\\text{count}(t, d))\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- If the count of term $t$ in document $d$ is 0, the formula results in 1 (to avoid zero counts).\n",
    "\n",
    "##### 6. **Complete Smoothed TF-IDF**\n",
    "\n",
    "The final **TF-IDF score with smoothing** combines both smoothed **TF** and **IDF** formulas:\n",
    "\n",
    "$$\n",
    "\\text{TF-IDF}_{\\text{smooth}}(t, d) = \\left( 1 + \\log(\\text{count}(t, d)) \\right) \\times \\log \\left( \\frac{N + 1}{df(t) + 1} \\right) + 1\n",
    "$$\n",
    "\n",
    "This formula:\n",
    "- Applies logarithmic smoothing to **TF**.\n",
    "- Applies smoothing to **IDF** to avoid division by zero and prevent extreme values for very common words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the smoothned version of TF-IDF, first create the function for TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Function to compute Term Frequency (TF) with smoothing\n",
    "def term_frequency(document, smoothing=True):\n",
    "    \"\"\"\n",
    "    Compute the Term Frequency (TF) for each term in a document with optional smoothing.\n",
    "    \n",
    "    :param document: List of words in a document.\n",
    "    :param smoothing: Boolean flag for applying Laplace smoothing.\n",
    "    :return: Dictionary with term frequency for each term.\n",
    "    \"\"\"\n",
    "    term_count = Counter(document)\n",
    "    total_terms = len(document)\n",
    "    \n",
    "    tf = {}\n",
    "    \n",
    "    # Count the frequency of each term in the document\n",
    "    for term, count in term_count.items():\n",
    "    # Apply Laplace smoothing if specified\n",
    "        if smoothing:\n",
    "            tf[term] = (count + 1) / (total_terms + len(term_count))\n",
    "        else:\n",
    "            tf[term] = count / total_terms\n",
    "    \n",
    "    return tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a function for IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Function to compute Inverse Document Frequency (IDF)\n",
    "def inverse_document_frequency(documents):\n",
    "    \"\"\"\n",
    "    Compute the Inverse Document Frequency (IDF) for each term in the corpus.\n",
    "    \n",
    "    :param documents: List of documents (each document is a list of words).\n",
    "    :return: Dictionary with inverse document frequency for each term.\n",
    "    \"\"\"\n",
    "    total_documents = len(documents)\n",
    "    document_frequency = {}\n",
    "\n",
    "    # Count how many documents each term appears in\n",
    "    for document in documents:\n",
    "        unique_terms = set(document)\n",
    "        for term in unique_terms:\n",
    "            document_frequency[term] = document_frequency.get(term, 0) + 1\n",
    "\n",
    "    # Calculate IDF for each term\n",
    "    idf = {}\n",
    "    for term, count in document_frequency.items():\n",
    "        idf[term] = math.log(total_documents / (1 + count))\n",
    "\n",
    "    \n",
    "    return idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute TF-IDF for each term in each document\n",
    "def tfidf_features(documents, smoothing=True):\n",
    "    \"\"\"\n",
    "    Compute the TF-IDF for each term in each document with optional smoothing on TF.\n",
    "    \n",
    "    :param documents: List of documents (each document is a list of words).\n",
    "    :param smoothing: Boolean flag for applying Laplace smoothing to TF.\n",
    "    :return: List of TF-IDF vectors for each document.\n",
    "    \"\"\"\n",
    "    # Step 1: Compute IDF for the corpus\n",
    "    idf = inverse_document_frequency(documents)\n",
    "    tfidf_vectors = []\n",
    "    \n",
    "    # Step 2: Compute TF for each document and then compute TF-IDF\n",
    "    for document in documents:\n",
    "        tf = term_frequency(document, smoothing)\n",
    "        tfidf_vector = {}\n",
    "        \n",
    "        for term in document:\n",
    "            tfidf_vector[term] = tf[term] * idf.get(term, 0)\n",
    "\n",
    "        tfidf_vectors.append(tfidf_vector)\n",
    "    \n",
    "    return tfidf_vectors, list(idf.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class NaiveBayes:\n",
    "    def __init__(self, method='bow'):\n",
    "        self.method = method\n",
    "        self.class_probs = {}\n",
    "        self.feature_probs = defaultdict(lambda: defaultdict(float))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        class_counts = defaultdict(int)\n",
    "        for label in y:\n",
    "            class_counts[label] += 1\n",
    "\n",
    "        total_documents = len(y)\n",
    "        for label, count in class_counts.items():\n",
    "            self.class_probs[label] = count / total_documents\n",
    "\n",
    "        feature_counts = defaultdict(lambda: defaultdict(int))\n",
    "        for i, document in enumerate(X):\n",
    "            label = y[i]\n",
    "            for word in document:\n",
    "                feature_counts[label][word] += 1\n",
    "\n",
    "        for label, word_counts in feature_counts.items():\n",
    "            total_words_in_class = sum(word_counts.values()) + len(word_counts)\n",
    "            for word, count in word_counts.items():\n",
    "                self.feature_probs[label][word] = (count + 1) / total_words_in_class\n",
    "\n",
    "    import numpy as np\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for document in X:\n",
    "            class_scores = {}\n",
    "            for label in self.class_probs:\n",
    "                score = np.log(self.class_probs[label])\n",
    "                if self.method == 'bow':\n",
    "                    for word in document:\n",
    "                        score += np.log(self.feature_probs[label].get(word, 1e-5))\n",
    "                elif self.method == 'tfidf':\n",
    "                    if isinstance(document, dict):  # Check if it's a dictionary for TF-IDF\n",
    "                        for word, tfidf_value in document.items():\n",
    "                            score += np.log(tfidf_value * self.feature_probs[label].get(word, 1e-5) if tfidf_value > 0 else 1e-5)\n",
    "                class_scores[label] = score\n",
    "\n",
    "            predicted_class = max(class_scores, key=class_scores.get)\n",
    "            predictions.append(predicted_class)\n",
    "\n",
    "        return np.array(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 5 5 ... 5 5 5]\n"
     ]
    }
   ],
   "source": [
    "# Example usage for TF-IDF\n",
    "nb = NaiveBayes(method='tfidf')\n",
    "nb.fit(X_train, y_train)\n",
    "predictions = nb.predict(X_test)\n",
    "print() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy for BoW and TFDIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    correct = np.sum(y_true == y_pred)\n",
    "    return correct / len(y_true)\n",
    "\n",
    "nb_bow = NaiveBayes(method='bow')\n",
    "nb_bow.fit(X_train, y_train)\n",
    "predictions_bow = nb_bow.predict(X_test)\n",
    "accuracy_bow = accuracy(y_test, predictions_bow)\n",
    "print(f\"Accuracy (BoW): {accuracy_bow * 100:.2f}%\")\n",
    "\n",
    "nb_tfidf = NaiveBayes(method='tfidf')\n",
    "nb_tfidf.fit(X_train, y_train)\n",
    "predictions_tfidf = nb_tfidf.predict(X_test)\n",
    "accuracy_tfidf = accuracy(y_test, predictions_tfidf)\n",
    "print(f\"Accuracy (TF-IDF): {accuracy_tfidf * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{"cells":[{"cell_type":"markdown","metadata":{"id":"lCEfPA7k0wVy"},"source":["## Naive Bayes and Vector Embeddings"]},{"cell_type":"markdown","metadata":{"id":"kb_XFc-t0wV1"},"source":["### Importing the data and libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"bE-m35uU0wV2","executionInfo":{"status":"ok","timestamp":1735801410990,"user_tz":-330,"elapsed":7808,"user":{"displayName":"Integration S","userId":"03211167078800808410"}}},"outputs":[],"source":["from sklearn.datasets import fetch_20newsgroups\n","import numpy as np\n","from collections import defaultdict"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cfFKUcyb0wV5","executionInfo":{"status":"ok","timestamp":1735801426068,"user_tz":-330,"elapsed":15083,"user":{"displayName":"Integration S","userId":"03211167078800808410"}},"outputId":"a2e08066-4d05-40d9-b2d5-8f20122691fd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Categories: ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"]}],"source":["# Load the 20 newsgroups dataset\n","newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n","\n","# Split into training and test sets\n","X_train, X_test, y_train, y_test = newsgroups.data[:1000], newsgroups.data[1000:], newsgroups.target[:1000], newsgroups.target[1000:]\n","\n","# Categories for evaluation\n","categories = newsgroups.target_names\n","\n","print(f\"Categories: {categories}\")"]},{"cell_type":"markdown","metadata":{"id":"xgMP14IP0wV7"},"source":["### Preprocessing"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"zHSdQoZq0wV7","executionInfo":{"status":"ok","timestamp":1735801426068,"user_tz":-330,"elapsed":6,"user":{"displayName":"Integration S","userId":"03211167078800808410"}}},"outputs":[],"source":["import string\n","import re\n","def preprocess_text(text):\n","    \"\"\"Convert text to lowercase and remove punctuation and non alpha numeric characters using regex\"\"\"\n","    text = text.lower()\n","    text = re.sub(r'[^a-z0-9\\s]', '', text)\n","\n","    return text\n","\n","# Apply preprocessing to training data\n","X_train = [preprocess_text(text) for text in X_train]\n","X_test = [preprocess_text(text) for text in X_test]\n"]},{"cell_type":"markdown","metadata":{"id":"kY-tAYIj0wV-"},"source":["### Feature Extraction"]},{"cell_type":"markdown","metadata":{"id":"ULUVp-CA0wWB"},"source":["Here create 2 functions: Bag of Words and TF-IDF"]},{"cell_type":"markdown","metadata":{"id":"7RQbHuVh0wWC"},"source":["##### **Bag of Words (BoW)**"]},{"cell_type":"markdown","metadata":{"id":"Q9nHTfFk0wWD"},"source":["First create the vocabulary for the bag of words, in layman terms get the set of all unique words"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"vKSxKjAn0wWD","executionInfo":{"status":"ok","timestamp":1735801426893,"user_tz":-330,"elapsed":830,"user":{"displayName":"Integration S","userId":"03211167078800808410"}}},"outputs":[],"source":["def create_vocabulary(text_data):\n","    \"\"\"Create a vocabulary from the provided text data.\"\"\"\n","    vocabulary = set()\n","    for text in text_data:\n","        words = text.split()\n","        vocabulary.update(words)\n","    return list(vocabulary)\n","\n","# Create vocabulary from the training data\n","vocabulary = create_vocabulary(X_train)"]},{"cell_type":"markdown","metadata":{"id":"NGgLN0Xg0wWD"},"source":["Next create a function to compute the number of occurences of that word given the vocabulary and text, return a dictionary"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"zkgiaEoW0wWD","executionInfo":{"status":"ok","timestamp":1735801426893,"user_tz":-330,"elapsed":32,"user":{"displayName":"Integration S","userId":"03211167078800808410"}}},"outputs":[],"source":["def BoW(text, vocabulary):\n","    \"\"\"Convert a text document into a vector based on the vocabulary.\"\"\"\n","    word_count = {word: 0 for word in vocabulary}\n","    words = text.split()\n","    for word in words:\n","        if word in word_count:\n","            word_count[word] += 1\n","    return list(word_count.values())"]},{"cell_type":"markdown","metadata":{"id":"aQw7qqvN0wWE"},"source":["#### **TF-IDF**"]},{"cell_type":"markdown","metadata":{"id":"aZBdww0A0wWE"},"source":["##### Theory"]},{"cell_type":"markdown","metadata":{"id":"c1W2DSVl0wWE"},"source":["**TF-IDF (Term Frequency-Inverse Document Frequency)** is a numerical statistic used to evaluate the importance of a word within a document relative to a collection (or corpus) of documents. It is commonly used in text mining and information retrieval tasks such as text classification, clustering, and document retrieval.\n","\n","##### 1. **Term Frequency (TF)**\n","\n","Term Frequency measures how frequently a term (word) appears in a document. It is calculated as:\n","\n","$$\n","\\text{TF}(t, d) = \\frac{\\text{Number of times term t appears in document d}}{\\text{Total number of terms in document d}}\n","$$\n","\n","##### 2. **Inverse Document Frequency (IDF)**\n","\n","Inverse Document Frequency measures how important a term is in the entire corpus. It reduces the weight of common words that appear in many documents. The IDF is calculated as:\n","\n","$$\n","\\text{IDF}(t) = \\log \\frac{N}{df(t)}\n","$$\n","\n","- Where:\n","  - $ N $ is the total number of documents in the corpus.\n","  - $ df(t) $ is the number of documents that contain the term \\$$t \\).\n","\n","##### 3. **TF-IDF Score**\n","\n","The **TF-IDF score** is the product of the Term Frequency (TF) and Inverse Document Frequency (IDF):\n","\n","$$\n","\\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\times \\text{IDF}(t)\n","$$\n","\n","- This score reflects the importance of a term in a document relative to its rarity across the entire corpus.\n","\n","\n","##### 4. **Smoothed IDF (Inverse Document Frequency)**\n","\n","The formula for **IDF** can be smoothed by adding a constant to the document frequency count in the denominator. This avoids division by zero and prevents excessively high values for terms that appear in most documents.\n","\n","**Smoothed IDF Formula:**\n","\n","$$\n","\\text{IDF}_{\\text{smooth}}(t) = \\log \\left( \\frac{N + 1}{df(t) + 1} \\right) + 1\n","$$\n","\n","Where:\n","- $ N $ = Total number of documents in the corpus.\n","- $ df(t) $ = Number of documents that contain the term $ t $.\n","- The `+1` in both the numerator and denominator ensures that the term frequency is never zero.\n","\n","##### 5. **Smoothed TF (Term Frequency)**\n","\n","A common smoothing technique for **Term Frequency** is to apply **logarithmic scaling**. Instead of using the raw term count, the logarithm of the term frequency is used to downscale the impact of very frequent terms.\n","\n","**Smoothed TF Formula:**\n","\n","$$\n","\\text{TF}_{\\text{smooth}}(t, d) = 1 + \\log(\\text{count}(t, d))\n","$$\n","\n","Where:\n","- If the count of term $t$ in document $d$ is 0, the formula results in 1 (to avoid zero counts).\n","\n","##### 6. **Complete Smoothed TF-IDF**\n","\n","The final **TF-IDF score with smoothing** combines both smoothed **TF** and **IDF** formulas:\n","\n","$$\n","\\text{TF-IDF}_{\\text{smooth}}(t, d) = \\left( 1 + \\log(\\text{count}(t, d)) \\right) \\times \\log \\left( \\frac{N + 1}{df(t) + 1} \\right) + 1\n","$$\n","\n","This formula:\n","- Applies logarithmic smoothing to **TF**.\n","- Applies smoothing to **IDF** to avoid division by zero and prevent extreme values for very common words.\n"]},{"cell_type":"markdown","metadata":{"id":"cJIzrA-D0wWF"},"source":["##### Code"]},{"cell_type":"markdown","metadata":{"id":"8_yOvA5G0wWF"},"source":["We will be using the smoothned version of TF-IDF, first create the function for TF"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"Bagd5Zm10wWF","executionInfo":{"status":"ok","timestamp":1735801426893,"user_tz":-330,"elapsed":30,"user":{"displayName":"Integration S","userId":"03211167078800808410"}}},"outputs":[],"source":["from collections import Counter\n","\n","# Function to compute Term Frequency (TF) with smoothing\n","def term_frequency(document, smoothing=True):\n","    \"\"\"\n","    Compute the Term Frequency (TF) for each term in a document with optional smoothing.\n","\n","    :param document: List of words in a document.\n","    :param smoothing: Boolean flag for applying Laplace smoothing.\n","    :return: Dictionary with term frequency for each term.\n","    \"\"\"\n","    term_count = Counter(document)\n","    total_terms = len(document)\n","\n","    tf = {}\n","\n","    # Count the frequency of each term in the document\n","    for term, count in term_count.items():\n","    # Apply Laplace smoothing if specified\n","        if smoothing:\n","            tf[term] = (count + 1) / (total_terms + len(term_count))\n","        else:\n","            tf[term] = count / total_terms\n","\n","    return tf"]},{"cell_type":"markdown","metadata":{"id":"A7-W48Q-0wWF"},"source":["Now create a function for IDF"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"ZJ4-f3Fb0wWF","executionInfo":{"status":"ok","timestamp":1735801426893,"user_tz":-330,"elapsed":29,"user":{"displayName":"Integration S","userId":"03211167078800808410"}}},"outputs":[],"source":["import math\n","\n","# Function to compute Inverse Document Frequency (IDF)\n","def inverse_document_frequency(documents):\n","    \"\"\"\n","    Compute the Inverse Document Frequency (IDF) for each term in the corpus.\n","\n","    :param documents: List of documents (each document is a list of words).\n","    :return: Dictionary with inverse document frequency for each term.\n","    \"\"\"\n","    total_documents = len(documents)\n","    document_frequency = {}\n","\n","    # Count how many documents each term appears in\n","    for document in documents:\n","        unique_terms = set(document)\n","        for term in unique_terms:\n","            document_frequency[term] = document_frequency.get(term, 0) + 1\n","\n","    # Calculate IDF for each term\n","    idf = {}\n","    for term, count in document_frequency.items():\n","        idf[term] = math.log(total_documents / (1 + count))\n","\n","\n","    return idf"]},{"cell_type":"markdown","metadata":{"id":"wfOEsllI0wWG"},"source":["Combining"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"ey7a0pdD0wWG","executionInfo":{"status":"ok","timestamp":1735801426894,"user_tz":-330,"elapsed":27,"user":{"displayName":"Integration S","userId":"03211167078800808410"}}},"outputs":[],"source":["# Function to compute TF-IDF for each term in each document\n","def tfidf_features(documents, smoothing=True):\n","    \"\"\"\n","    Compute the TF-IDF for each term in each document with optional smoothing on TF.\n","\n","    :param documents: List of documents (each document is a list of words).\n","    :param smoothing: Boolean flag for applying Laplace smoothing to TF.\n","    :return: List of TF-IDF vectors for each document.\n","    \"\"\"\n","    # Step 1: Compute IDF for the corpus\n","    idf = inverse_document_frequency(documents)\n","    tfidf_vectors = []\n","\n","    # Step 2: Compute TF for each document and then compute TF-IDF\n","    for document in documents:\n","        tf = term_frequency(document, smoothing)\n","        tfidf_vector = {}\n","\n","        for term in document:\n","            tfidf_vector[term] = tf[term] * idf.get(term, 0)\n","\n","        tfidf_vectors.append(tfidf_vector)\n","\n","    return tfidf_vectors, list(idf.keys())"]},{"cell_type":"markdown","metadata":{"id":"iIhezakW0wWG"},"source":["#### Implementing Naive Bayes"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"nK3dkMEE0wWH","executionInfo":{"status":"ok","timestamp":1735802324303,"user_tz":-330,"elapsed":545,"user":{"displayName":"Integration S","userId":"03211167078800808410"}}},"outputs":[],"source":["class NaiveBayes:\n","    def __init__(self, method='bow'):\n","        self.method = method\n","        self.class_probs = {}\n","        self.feature_probs = defaultdict(lambda: defaultdict(float))\n","        self.vocabulary = set()\n","        self.idf = {}\n","\n","    def fit(self, X, y):\n","        if self.method == 'bow':\n","            self.vocabulary = set(word for doc in X for word in doc.split())  # BoW vocabulary\n","        elif self.method == 'tfidf':\n","            self.idf = inverse_document_frequency(X)  # Use entire corpus to compute IDF\n","            X, _ = tfidf_features(X)  # Compute TF-IDF vectors for the corpus\n","\n","        class_counts = defaultdict(int)\n","        for label in y:\n","            class_counts[label] += 1\n","\n","        total_documents = len(y)\n","        for label, count in class_counts.items():\n","            self.class_probs[label] = count / total_documents\n","\n","        feature_counts = defaultdict(lambda: defaultdict(int))\n","        for i, document in enumerate(X):\n","            label = y[i]\n","            if self.method == 'bow':\n","                for word in document.split():  # For BoW, it's a simple list of words\n","                    feature_counts[label][word] += 1\n","            elif self.method == 'tfidf':\n","                if isinstance(document, dict):  # For TF-IDF, it's a dictionary of word:tf-idf pairs\n","                    for word, tfidf_value in document.items():\n","                        feature_counts[label][word] += tfidf_value\n","\n","        for label, word_counts in feature_counts.items():\n","            total_words_in_class = sum(word_counts.values()) + len(word_counts)\n","            for word, count in word_counts.items():\n","                self.feature_probs[label][word] = (count + 1) / total_words_in_class\n","\n","    def predict(self, X):\n","        predictions = []\n","        for document in X:\n","            class_scores = {}\n","            for label in self.class_probs:\n","                score = np.log(self.class_probs[label])\n","                if self.method == 'bow':\n","                    for word in document.split():  # For BoW, it's a simple list of words\n","                        score += np.log(self.feature_probs[label].get(word, 1e-5))\n","                elif self.method == 'tfidf':\n","                    if isinstance(document, dict):  # For TF-IDF, it's a dictionary of word:tf-idf pairs\n","                        for word, tfidf_value in document.items():\n","                            score += np.log(tfidf_value * self.feature_probs[label].get(word, 1e-5) if tfidf_value > 0 else 1e-5)\n","                class_scores[label] = score\n","\n","            predicted_class = max(class_scores, key=class_scores.get)\n","            predictions.append(predicted_class)\n","\n","        return np.array(predictions)"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CtApakqJ0wWH","executionInfo":{"status":"ok","timestamp":1735801427548,"user_tz":-330,"elapsed":661,"user":{"displayName":"Integration S","userId":"03211167078800808410"}},"outputId":"87ecfaca-cf18-4484-e8a0-6a0a4634afa6"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n"]}],"source":["# Example usage for TF-IDF\n","nb = NaiveBayes(method='tfidf')\n","nb.fit(X_train, y_train)\n","predictions = nb.predict(X_test)\n","print()"]},{"cell_type":"markdown","metadata":{"id":"qGb2LrdJ0wWI"},"source":["#### Accuracy for BoW and TFDIF"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fE4_tM150wWI","executionInfo":{"status":"ok","timestamp":1735802430168,"user_tz":-330,"elapsed":100489,"user":{"displayName":"Integration S","userId":"03211167078800808410"}},"outputId":"effa519d-640e-4cbb-8b62-fceb84b30cec"},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy (BoW): 48.24%\n","Accuracy (TF-IDF): 5.16%\n"]}],"source":["import numpy as np\n","\n","def accuracy(y_true, y_pred):\n","    correct = np.sum(y_true == y_pred)\n","    return correct / len(y_true)\n","\n","nb_bow = NaiveBayes(method='bow')\n","nb_bow.fit(X_train, y_train)\n","predictions_bow = nb_bow.predict(X_test)\n","accuracy_bow = accuracy(y_test, predictions_bow)\n","print(f\"Accuracy (BoW): {accuracy_bow * 100:.2f}%\")\n","\n","nb_tfidf = NaiveBayes(method='tfidf')\n","nb_tfidf.fit(X_train, y_train)\n","predictions_tfidf = nb_tfidf.predict(X_test)\n","accuracy_tfidf = accuracy(y_test, predictions_tfidf)\n","print(f\"Accuracy (TF-IDF): {accuracy_tfidf * 100:.2f}%\")\n"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"73dGgUUt0wWI","executionInfo":{"status":"ok","timestamp":1735802856595,"user_tz":-330,"elapsed":11448,"user":{"displayName":"Integration S","userId":"03211167078800808410"}},"outputId":"d39b1f5e-8744-455f-a953-46d2eddd9bf5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy (TF-IDF with vectorizer): 5.16%\n"]}],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# Define a TF-IDF Vectorizer with parameters\n","tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n","\n","# Fit the model with training data and transform both train and test data\n","X_train_tfidf = tfidf_vectorizer.fit_transform(X_train).toarray()  # Convert sparse matrix to dense\n","X_test_tfidf = tfidf_vectorizer.transform(X_test).toarray()  # Convert sparse matrix to dense\n","\n","# Train Naive Bayes with the new TF-IDF features\n","nb_tfidf = NaiveBayes(method='tfidf')\n","nb_tfidf.fit(X_train_tfidf, y_train)\n","predictions_tfidf = nb_tfidf.predict(X_test_tfidf)\n","accuracy_tfidf = accuracy(y_test, predictions_tfidf)\n","print(f\"Accuracy (TF-IDF with vectorizer): {accuracy_tfidf * 100:.2f}%\")\n"]},{"cell_type":"code","source":[],"metadata":{"id":"PuXllk6u679W"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}